---
title: "Exploring Invertibility With Binary Matrices"
author: "Jack Norman"
output: 
  html_document:
    style: united
    toc: true
    toc_depth: 4
    highlight: pygments
---

```{r, echo = FALSE}
setwd("/Users/jacknorman1/Documents/USF/Bootcamp/Linear Algebra/Project/invertible_binary_matrices")
require(knitr)
library(MASS)
library(ggplot2)
library(grid)
library(rmarkdown)
source("multiplot.R")
```

### Introduction
Binary matrices occur in application quite often. Two examples of binary matrices that are quite common are permutation matrices and adjacency matrices. Being approached by this problem of determining or estimating the proportion of binary matrices that are invertible for different matrix sizes, I was surprised to learn that there isn't already a known relationship between the number of unique binary matrices and the number of those that are invertibles. In other words, at first glance, it doesn't seem like the proportion of invertible matrices would be a mystery.

Approaching this problem both from the brute force perspective and the sampling perspective really intrigued me. For the brute force method, the value that attain is the exact proportion. Something about knowing the exact proportion almost seems magical, as if it's part of the answer to life!

### Brute Force Using Python
```{r, engine="python", highlight=TRUE}
import itertools
import numpy as np
import time

def percent_invertible_bf(n):
    '''Calculates the exact proportion of invertible matrices

    Args:
        n: the size of the matrix (nxn)

    Returns:
        The proportion of invertible matrices
    '''
    
    sum = 0.
    for i in itertools.product(range(2), repeat = n**2):
       mat = np.matrix(i).reshape(n, n)
       det = np.linalg.det(mat)
       sum += int(det != 0)
    return sum / 2**n**2
```

```{r kable, echo = FALSE}
invertible_info <- data.frame(n = c(2, 3, 4, 5), 
		   Num_Unique_Matrices = c(16, 512, 65536, 33554432),
		   Percent_Invertible = c(.375, .33984375, .34423828125, .372955799103),
		   Seconds_to_Complete = c(.00039, .00916, 1.2962, 649.57))

kable(invertible_info, row.names = FALSE, align = 'l', format = 'markdown')
```

As one can see, while I am able to obtain these "magical" numbers of .375, .33984375, .34423828125, and .372955799103, I run in a barrier extremely quickly. The number of unique matrices explodes at a rate of `2**n**2`, i.e. from 16 to 512 to 65,536 to 33,554,432 different matrices! For the 5x5 matrix, my program took almost eleven minutes to complete. I wanted to figure out, using the same brute force method as above, how long my program would expect to take calculating all the 6x6 matrices. And 7x7 matrices? 

To do this, I looked at the relationship between the `Num_Unique_Matrices` and `Seconds_to_Complete` numbers in the table above. After trying out a few transformations to see what kind of relationship existed between the two, I came across a nearly perfect fit: The `log(Seconds_to_Complete)` as a function of `log(Num_Unique_Matrices)`, seen in the plot below.

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 5, fig.width = 6}
nums = invertible_info$Num_Unique_Matrices
secs = invertible_info$Seconds_to_Complete

p <- ggplot() + geom_point(aes(x = log(nums), y = log(secs), size = 3)) +
	ggtitle("Log of Number of Seconds\nAs a Function of\nLog of Number of Unique Matrices") +
	xlab("log(Number of Unique Matrices)") +
	ylab("log(Number of Seconds)")

p
```

In order to calculate an estimate of how long this program would run for 6x6, I can use the formula, attained from `lm()` in R, `log(# of Seconds) = .98968 * log(# of Unique Matrices) - 10.71259`. Of course, this will be extrapolating from the best fit line, so I will have to be careful. But because the R^2 value is so extreme (.9995), I expect the line will persist. The expected number of seconds according to this fitted linear model of the transformed independent and dependent variables for a 6x6 matrix is 1,182,684 which is over 13 days of nonstop running of this program. 

At this point, what appealed to me was coming up with an algorithm that comes up with an estimate for the proportion of matrices that invertible for a given size, n, and a given bias towards picking a 0, p. I used R for my sampling algorithm.

### Sampling Using R
```{r, engine="R"}
library(MASS)

generateBinaryMatrix <- function(n, p = .5) {
	# Generate one random binary matrix
	#
	# Args:
	#	n: the size of the matrix (nxn)
	#	p: the probability at which to sample zeros (i.e., 1 - p will be
	#		the rate at which ones are sampled)
	# Returns:
	#	An nxn matrix
	#
	data = sample(c(0, 1), n^2, replace = TRUE, prob = c(p, 1 - p))
	A = matrix(data, n)
	return(A)
}

inverseIteration <- function(n, p) {
	# Determines the invertibility of one matrix
	#
	# Args:
	#	n: the size of the matrix (nxn)
	#	p: the probability at which to sample zeros (i.e., 1 - p will be
	#		the rate at which ones are sampled)
	# Returns:
	#	A logical value indicating the invertibility of the matrix
	#
	A = generateBinaryMatrix(n, p)
	A_det <- det(A)
	return(A_det != 0)
}

runner <- function(B = 100000, n = 5, p = .5) {
	# Iterates over inverseIteration many times.
	#
	# Args:
	#	B: the number of random binary matrices to create
	#	n: the size of the matrix (nxn)
	#	p: the probability at which to sample zeros (i.e., 1 - p will be
	#		the rate at which ones are sampled)
	# Returns:
	#	The mean proportion of invertible matrices
	#
	mean(vapply(1:B, function(x) {
		inverseIteration(n, p)
	}, logical(1L)))
}


grid_runner <- function(grid, B) {
	# Runs many sample for a grid of values for n and p
	#
	# Args:
	#	grid: a data.frame with combinatorial values for n and p
	#	B: the number of random binary matrices to create
	#
	# Returns:
	#	A data.frame with variables n, p, and isInverse which represent
	#		the size of the matrices, the proportion bias towards zeros,
	#		and the proprtion of the invertible matrices from the smaple.
	#
	df <- as.data.frame(t(sapply(1:nrow(grid), function(x) {
		n <- grid[x, 1]
		p <- grid[x, 2]
		res <- runner(B, n, p)
		c(n, p, res)
	})))
	names(df) <- c("n", "p", "isInverse")
	df
}

```

The functions above create binary matrices with a given size of n, the size of the matrix, and p, the bias. This is done a specified B number of times for every combinatino of n and p. I ran simulations for 551 different combinations of n and p. For n, I used the values 2 through 30, and for p, I used the values .05 to .95. I omitted probabilities 0 and 1 because those matrices will never be invertible due to linear dependence (every row/column will be all zeros or all ones). After running B = 10,000 iterations for all 551 of these scenarios, I produced the following plot.

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 5, fig.width = 6, fig.align = "center"}
load(file = "grid_sol_2_30_1_05_95_05_10000.Rda")
B = 10000

# note this a smoother so probs go above 1
p <- ggplot() + 
	stat_smooth(data = grid_sol2[grid_sol2$n <= 20, ], 
	 		    aes(x = p, y = isInverse, color = factor(n)),
	 		    se = FALSE) +
	ggtitle(paste("Invertibility by Size of Matrix and Bias\nB = ", B)) +
	xlab("Bias") +
	ylab("Proportion Invertible") +
	scale_x_continuous(breaks = seq(0, 1, .1)) + 
	scale_colour_discrete(name = "n")

p
```

In the plot above, the mode of each distribution is clearly at or very near to .5. This intuitively makes sense because it means the matrices produced are least likely to duplicate rows/columns. The mode being near .5 is true for all sizes of n except for n = 2, 3, and 4. Also, notice that as n increases in size, the higher the proportion of the sampled matrices are invertible. Again, this makes sense intuitively because with larger rows, there is less of a chance of obtaining a linear combination of another row. At the maximum of the very large matrices, the plot shows the proportion of invertibility is greater than one, but this is simply due to the smoothing error of this density plot.

One interesting pattern in the graph above is if you look at the height differences between distributions when the bias is 0.5. As n enlarges from 5 to 10, the difference increases, and as n continues to enlarge from 10 and on, the difference decreases. In other words, the rate at which the proportion of invertibility increases as n increases is variable!

Seen below are the shapes of the distributions of the invertibility against the size of the matrix.

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 5, fig.width = 6, fig.align = "center"}
q <- ggplot() + 
	geom_smooth(data = grid_sol2, 
	 		   aes(x = n, y = isInverse, colour = factor(p)),
	 		   se = FALSE) +
	ggtitle(paste("Invertibility by Size of Matrix and Bias\nB = ", B)) +
	xlab("Size of Matrix") +
	ylab("Proportion Invertible") +
	coord_cartesian(xlim = c(2, 25)) +
	scale_x_continuous(breaks = seq(2, 24, 2)) + 
	scale_colour_discrete(name = "Bias")

q
```

One thing that's noticeable from this graph is that as the bias extends towards the extremes, i.e. as it gets closer to 0 or 1, the rate at which the invertibility approaches 1 is a lot slower. In fact, there is a bit of a sweeping motion. In other words, the shapes of each bias are fairly simiilar: when the matrices invert between, say, 25% and 75%, the rate at which they do so is pretty much identical (the lines are parallel).

This plot is nice to see the sweeping motion of the distributions, but it is difficult to discern one line from the next. To declutter the graph, I used faceting in order to look at the shape of the invertibility as n increases individually for each bias, seen below.

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 6, fig.width = 12, fig.align = "center"}
s <- ggplot() + 
	geom_point(data = grid_sol2[grid_sol2$n <= 30, ], 
	 		    aes(x = n, y = isInverse, color = factor(n))) +
	 		    facet_wrap( ~ p) +
	ggtitle(paste("Invertibility by Size of Matrix and Bias\nB = ", B)) +
	xlab("n") +
	ylab("Proportion Invertible") +
	guides(col = guide_legend(ncol = 2)) +
	scale_x_discrete(breaks = seq(2, 30, 4)) + 
	scale_colour_discrete(name = "n")

s
```